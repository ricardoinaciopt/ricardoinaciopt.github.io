<!DOCTYPE html>
<html data-page="blog-post">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="shortcut icon" href="/assets/images/favi.webp" />
  <title>R_______________________I</title>
  <link rel="stylesheet" href="/style.css" />
</head>

<body>
  <div id="content">
    <h1 id="home">ricardoinacio - blog</h1>
    <div id="navbar" class="glass">
      <a href="/">home</a>
      <a href="/blog">blog</a>
      <a href="/about">world</a>
      <a href="mailto:ricardoinacio@mailbox.org">contact</a>
    </div>

    <div id="posts">
      <h1 id="to-teach-a-neuron">To Teach a Neuron</h1>
      <h5>26/03/2024</h5>
      <p>
        If we really think about it, every aspect of what we do in our daily lives produces an outcome; every little act
        and
        interaction, be it physical touch or a simple conversation, has an effect on something else. These interactions
        can be
        simplified and thought of as if they were functions, receiving our contributions as input, being weighed by how
        much of
        an impact they have, and producing the outcomes as output. Actually, the spectrum of things we can think of as
        functions is almost unlimited. That's why having a model that has the possibility of representing (or at least
        approximating)
        them would
        be invaluable to solving the challenging problems we face on a regular basis.
      </p>

      <img class="postimage" src="../post_files/as_function.png" alt="actions as functions">

      <p>
        That is where neural networks enter the picture, since that's exactly what they do! If you can design a NN that
        is
        sufficiently complex, any desired function could be hypothetically mathematically approximated. To understand
        how these mechanisms
        actually work, let's push the complexity back a bit and focus on a smaller scale, the unit: the neuron.
      </p>

      <img class="postimage" src="../post_files/neuron.png" alt="the neuron">

      <p>
        These simply take an input <code>x</code>, which gets weighted by the connection's strength,
        <code>w⋅x</code>,
        and a
        bias (<code>b</code>) is added to help represent slightly more complex issues (e.g., when the desired function
        does not cross
        the
        origin), adding a bit of flexibility to the network, so that it can learn better. Lastly,
        this weighted and likely biased input, <code>z = wx+b</code> passes through an activation
        function, where, given
        that the value is
        higher than a certain threshold, <code>a(z) > threshold,</code> the result can be sent forward, as it's
        considered relevant. Real networks are made by multiple layers of these neurons, each with multiple neurons.
        Every one receives as input the outputs from all neurons from the previous layers, again weighted by their
        connections. To keep complexity low, let´s image we only have one neuron per layer in our NN. Possible and
        common activation functions (that decide if values should keep propagating forward) are the Sigmoid (<math
          class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mrow>
              <mfrac>
                <mn>1</mn>
                <mrow>
                  <mn>1</mn>
                  <mo>+</mo>
                  <msup>
                    <mi>e</mi>
                    <mrow>
                      <mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mo>
                      <mi>x</mi>
                    </mrow>
                  </msup>
                </mrow>
              </mfrac>
            </mrow>
            <annotation encoding="application/x-tex">\frac{1}{1 + e^{-x}}</annotation>
          </semantics>
        </math>), the Hyperbolic Tangent (<math class="formula" xmlns="http://www.w3.org/1998/Math/MathML"
          display="inline">
          <semantics>
            <mrow>
              <mfrac>
                <mrow>
                  <msup>
                    <mi>e</mi>
                    <mi>x</mi>
                  </msup>
                  <mo>−</mo>
                  <msup>
                    <mi>e</mi>
                    <mrow>
                      <mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mo>
                      <mi>x</mi>
                    </mrow>
                  </msup>
                </mrow>
                <mrow>
                  <msup>
                    <mi>e</mi>
                    <mi>x</mi>
                  </msup>
                  <mo>+</mo>
                  <msup>
                    <mi>e</mi>
                    <mrow>
                      <mo form="prefix" stretchy="false" lspace="0em" rspace="0em">−</mo>
                      <mi>x</mi>
                    </mrow>
                  </msup>
                </mrow>
              </mfrac>
            </mrow>
            <annotation encoding="application/x-tex">\frac{e^x - e^{-x}}{e^x + e^{-x}}</annotation>
          </semantics>
        </math>), or the Rectified Linear Unit (<math class="formula" xmlns="http://www.w3.org/1998/Math/MathML"
          display="inline">
          <semantics>
            <mrow>
              <mrow>
                <mi>max</mi>
                <mo>⁡</mo>
              </mrow>
              <mo form="prefix" stretchy="false">(</mo>
              <mn>0</mn>
              <mo separator="true">,</mo>
              <mi>x</mi>
              <mo form="postfix" stretchy="false">)</mo>
            </mrow>
            <annotation encoding="application/x-tex">\max(0, x) </annotation>
          </semantics>
        </math>).

      </p>

      <p>Every single neuron that contributes to these complex systems, simply executes this basic procedure: take the
        input and produce an output. This way, in order to approximate any desired function, we can only
        change two
        values: the weights and biases. Initially, no network will be able to figure out what these two variables are
        supposed
        to look like, so we need to start them off with arbitrary values and change them iteratively until they are
        close
        enough to the “real” ones. That is what is meant by learning a function: finding the sweet spot of the weights
        and
        biases. To reach that spot, we need to test them and see how close or far from reality they actually are (maybe
        a
        metric would help to evaluate the situation). Let's use the Mean Squared Error (it's simple enough to be easily
        understood):
        <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mrow>
              <mfrac>
                <mn>1</mn>
                <mi>N</mi>
              </mfrac>
              <mrow>
                <munderover>
                  <mo movablelimits="false">∑</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>N</mi>
                </munderover>
              </mrow>
              <mo>(</mo>
              <msub>
                <mover>
                  <mi>y</mi>
                  <mo>^</mo>
                </mover>
                <mi>i</mi>
              </msub>
              <mo>-</mo>
              <msub>
                <mi>y</mi>
                <mi>i</mi>
              </msub>
              <msup>
                <mo>)</mo>
                <mn>2</mn>
              </msup>
            </mrow>
            <annotation encoding="application/x-tex">\frac{1}{N}\sum_{i=1}^{N}(ŷ_i-y_i)^2</annotation>
          </semantics>
        </math>
      </p>

      <p>Where <code>N</code> is the number of examples we have to train the network, <code>ŷ</code> is the predicted
        output our network
        gives, and <code>y</code>
        is the actual value the function we are trying to approximate would output (for every single example). Now that
        we have a metric to measure
        how far
        we still are, we can use it to define the <code>Cost</code> function. This function uses the chosen error
        metric, and by
        looking at
        it with respect to different parameters (as you may have guessed it already, the <code>weight</code> and the
        <code>bias</code>) tries to
        find
        the values that minimize the error (or cost), indicating a closer approximation to the "real" function. In this
        instance, the squared difference between the value of our
        function
        outputs,
        <code>ŷ</code>, and the real value <code>y</code> will be used.

        To formalize this, we use <code>gradients</code>, which give
        us a <code>vector</code>
        representative of
        how much the cost function changes with respect to a specific parameter (in other words, how sensitive it is to
        them).
        In this case, to get the gradient for the weight and bias parameters, we use
        <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mfrac>
              <mrow>
                <mi>∂</mi>
                <mi>C</mi>
              </mrow>
              <mrow>
                <mi>∂</mi>
                <mi>w</mi>
              </mrow>
            </mfrac>
            <annotation encoding="application/x-tex">\frac{∂C}{∂w}</annotation>
          </semantics>
        </math>
        and
        <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mfrac>
              <mrow>
                <mi>∂</mi>
                <mi>C</mi>
              </mrow>
              <mrow>
                <mi>∂</mi>
                <mi>b</mi>
              </mrow>
            </mfrac>
            <annotation encoding="application/x-tex">\frac{∂C}{∂b}</annotation>
          </semantics>
        </math>
        respectively.
      </p>

      <p>
        The question now is, how do we get these gradients? We have come a long way (in terms of steps) since we
        introduced the
        function's variable parameters <code>w</code> and <code>b</code>, until we reached the final outcome of the
        prediction <code>ŷ</code>. So, to
        compute this vector, we
        will use the
        <code>chain rule</code>.
      </p>

      <img class="postimage" src="../post_files/chain_rule.png" alt="chain rule">

      <p>
        There are a lot of different (and probably better) ways to explain it, but think of it as a chain of connections
        between
        directly adjacent values in the calculation of our approximated function's output, where in each step we
        consider how each component affects the final outcome. In other words, a chain of specific parameters
        influences, at each needed step for the final outcome. Let's recall the steps: We
        take an
        input <code>x</code>, multiply it by the connection's weight,
        <code>w</code>, and add the bias, <code>b</code>: <code>wx+b</code>. To keep it simple,
        let's call
        this weighted (and potentially biased) input <code>z</code>. Then, we take this value and pass it through the
        activation
        function
        <code>a(z)</code>, and it becomes our predicted resulting value <code>ŷ</code> (imagining this is the last
        layer, where this is the final prediction, since in reality, multiple hidden layers would have been introduced
        and passed through until this step).
        To finish, we just have to
        compute the error according
        to the
        chosen metric, regarding the actual value <code>y</code>:
        <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mrow>
              <mo>(</mo>
              <mover>
                <mi>y</mi>
                <mo>^</mo>
              </mover>
              <mo>-</mo>
              <mi>y</mi>
              <msup>
                <mo>)</mo>
                <mn>2</mn>
              </msup>
            </mrow>
            <annotation encoding="application/x-tex">(\hat{y} - y)^2</annotation>
          </semantics>
        </math>.
      </p>

      <p>
        Now that we have this pipeline constructed, we must simply start at the end (on the <code>Cost</code>), and
        reach the start
        <code>z =
        wx+b</code>, to obtain the sought-after parameters (<code>w</code> and <code>b</code>). To get to
        <code>w</code>, we use the chain rule, starting from
        the
        <code>Cost</code>, and going to the prediction (the last neuron's activation function output,
        <code>ŷ = a(z)</code>).
        Then, from the
        predicted
        output <code>ŷ</code>, to the activation function's input, <code>z</code>. And then, finally, we reach the
        desired variable <code>w</code>, from the direct computation of <code>z</code>. In
        each of
        these chain steps, when I say get from J to i, what I formally mean is to get the derivative of <code>J</code>
        with respect
        to <code>i</code>,
        formally, <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mfrac>
              <mrow>
                <mi>∂</mi>
                <mi>J</mi>
              </mrow>
              <mrow>
                <mi>∂</mi>
                <mi>i</mi>
              </mrow>
            </mfrac>
            <annotation encoding="application/x-tex">\frac{∂J}{∂i}</annotation>
          </semantics>
        </math>. So, to get what we are seeking, we must simply do
        <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mrow>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mi>C</mi>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <mi>w</mi>
                </mrow>
              </mfrac>
              <mo>=</mo>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mi>C</mi>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <mover>
                    <mi>y</mi>
                    <mo>^</mo>
                  </mover>
                </mrow>
              </mfrac>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mover>
                    <mi>y</mi>
                    <mo>^</mo>
                  </mover>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <mi>z</mi>
                </mrow>
              </mfrac>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mi>z</mi>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <mi>w</mi>
                </mrow>
              </mfrac>
            </mrow>
            <annotation encoding="application/x-tex">\frac{∂C}{∂w}=\frac{∂C}{∂ŷ}\frac{∂ŷ}{∂z}\frac{∂z}{∂w}</annotation>
          </semantics>
        </math> for the weight, and
        <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mrow>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mi>C</mi>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <mi>b</mi>
                </mrow>
              </mfrac>
              <mo>=</mo>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mi>C</mi>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <mover>
                    <mi>y</mi>
                    <mo>^</mo>
                  </mover>
                </mrow>
              </mfrac>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mover>
                    <mi>y</mi>
                    <mo>^</mo>
                  </mover>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <mi>z</mi>
                </mrow>
              </mfrac>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mi>z</mi>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <mi>b</mi>
                </mrow>
              </mfrac>
            </mrow>
            <annotation encoding="application/x-tex">\frac{∂C}{∂b}=\frac{∂C}{∂ŷ}\frac{∂ŷ}{∂z}\frac{∂z}{∂b}</annotation>
          </semantics>
        </math> for the bias.
      </p>

      <p>
        Remember, this is only for a single neuron per layer, so in a common network's environment, we need to aggregate
        the
        gradients of
        all neurons (available examples) in some way (e.g. averaging them).
      </p>

      <p>
        To make the neuron learn (at each step, slightly better) weight and bias values, we update them, considering the
        gradient of our cost function, which will decide the size and direction of the step to take. We should also pay
        attention to a
        special
        value called the learning rate (<code>α</code>). This small number is what we call a hyperparameter, which
        influences how rapidly
        the
        network adapts. To update each, respectively, we use:
        <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mrow>
              <msub>
                <mi>W</mi>
                <mi>j</mi>
              </msub>
              <mo>=</mo>
              <msub>
                <mi>W</mi>
                <mi>j</mi>
              </msub>
              <mo>−</mo>
              <mi>α</mi>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mi>C</mi>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <msub>
                    <mi>W</mi>
                    <mi>j</mi>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <annotation encoding="application/x-tex">W_j = W_j - \alpha \frac{\partial C}{\partial W_j} </annotation>
          </semantics>
        </math>
        and
        <math class="formula" xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
          <semantics>
            <mrow>
              <msub>
                <mi>b</mi>
                <mi>j</mi>
              </msub>
              <mo>=</mo>
              <msub>
                <mi>b</mi>
                <mi>j</mi>
              </msub>
              <mo>−</mo>
              <mi>α</mi>
              <mfrac>
                <mrow>
                  <mi>∂</mi>
                  <mi>C</mi>
                </mrow>
                <mrow>
                  <mi>∂</mi>
                  <msub>
                    <mi>b</mi>
                    <mi>j</mi>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
            <annotation encoding="application/x-tex">b_j = b_j - \alpha \frac{\partial C}{\partial b_j}</annotation>
          </semantics>
        </math>
      </p>



      <p>All things considered, let's just say that normally we would take big steps in the beginning and smaller steps
        as we
        reach lower Cost (aka Loss) function values, optimizing the parameters of our network, through a method known as
        <code>Gradient Descent</code>.
        This particular subject could be the sole topic of another post, so for the sake of brevity, let's
        simplify (a gross amount) this matter.
      </p>

      <img class="postimage" src="../post_files/gradient_descent.png" alt="gradient descent" s>
      <p>As mentioned before, it's expected that early on the <code>Cost</code> value is significant; that's why the
        gradient of
        the function, regarding the parameter (weight or bias), will be quite substantial (negative or positive, with
        the sign
        simply indicating the direction of the step).
        Then, we must update the values of said parameters, considering the learning rate (<code>α</code>), and use them
        on the
        next iterations to be once again updated if needed. The main idea is that by forcing a network to change these
        values, we can directly tell it how to individually change the connections between neurons, and how much
        each output should influence the outcome (reduce or increase).
      </p>
      <p>
        As it's observable in the example, as the <code>Cost</code> function is
        minimized, the gradients also
        become smaller (think of it as the inclination of the loss), until it reaches the
        point when the updates (steps) are no longer significant.
        That is
        when we
        stop, since we've reach the optimal (local or global) parameter value! Since most real problems won't be
        represented by a simple convex function, the <code>global minima</code> can't be guaranteed, thus the need for
        variations on this algorithm, to find the parameter values more effectively, such as
        <code>Stochastic Gradient Descent (SGD)</code> or
        <code>Mini-Batch Gradient Descent</code>.

      </p>
      <p>
        By doing this, paired with a careful design and architecture of our network,
        we can reach a point where the parameters we were seeking (weights and biases), allow us to very accurately
        deduce any
        parameterized function we intend to, and consequently, solve fairly complex problems.
      </p>

    </div>
  </div>
</body>
<script type="text/javascript" src="/main.js"></script>

</html>