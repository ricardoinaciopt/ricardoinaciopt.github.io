If we really think about it, every little act we engage with in our daily lives produces an outcome; every little action and interaction, be it physical touch or a simple conversation, has an effect on something else. These interactions can be simplified and thought of as if they were functions, receiving our contributions as input, and producing the outcomes as output. Actually, most of the things we can think of, can be represented as functions. That's why having a model that could possibly represent (or at least approximate) them, would be invaluable to solving the very challenging problems we face every day. 

That is where neural networks enter the picture, since that's exactly what they do! If you can design a NN that is sufficiently complex, any desired function could be hypothetically mathematically approximated. To understand how they actually work, let's put the complexity aside and focus on a smaller scale, the neuron. 


These networks are made by (in some cases, hundreds of thousands or even millions of) them, and they all function in the same way: take one or more input values, multiply them by their respective connection's weight, and sum them together, including a special value called the bias, which can influence its behavior. The value we get can then be used in the neuron's activation function, which will check if the value is above a certain threshold, in order to activate it. The value will then be sent to the next layer in the network, or outputted. As I have mentioned before, a function takes one or more inputs, and produces an output. This way, in order to approximate any desired function, we can only change two values: the weights and biases. Initially, no network will be able to figure out what these two variables are supposed to look like, so we need to start them off with arbitrary values, and change them iteratively until they are close enough to the “real” ones. That is what is meant by learning a function: finding the sweet spot of the weights and biases. To reach that spot, we need to test them, and see how close or far from reality they actually are (maybe a metric would help to evaluate the situation). Let's use the Mean Squared Error (it's simple enough to be easily understood), represented by  $\frac{1}{N}\sum_{i=1}^{N}(ŷ_i-y_i)^2$.

Where `N` is the number of examples we have to train the network, `ŷ` is the predicted output our network gives, and `y` is the actual value the function we are trying to approximate would output. Now that we have a metric to measure how far we still are, we can use it to define the cost function. This function, uses the chosen error metric, and by looking at it in respect to different parameters (as you may have guessed it already, the `weight` and the `bias`) tries to find the values that minimize the error. In this instance, the squared difference between the value of our function outputs, `ŷ`, and the real value `y` will be used. To formalize this, we use gradients, which gives us a vector representative of how much the cost function changes with respect to a specific parameter (in other words, how sensitive it is to them). In this case, to get the gradient for the weight and bias parameters, we use $\frac{∂C}{∂w}$ and $\frac{∂C}{∂b}$ respectively



The question now is, how do we get these gradients? We have come a long way (in terms of steps) since we introduced the weights, until we reached the final outcome of the prediction `ŷ`. So, to compute this vector, we will use the chain rule. 

There are a lot of different (and probably better) ways to explain it, but think of it as a chain of connections between directly adjacent values in the calculation of our approximated function's output. Let's remember the steps: We take an input `x`, we multiply it by the connection's weight, `w`, and add the bias, `b`: `wx+b`. To keep it simple, let's call this weighted (and potentially biased) input `z`. Then, we take this value and pass it through the activation function `a(z)`, and it becomes our predicted resulting value `ŷ`. To finish, we just have to compute the error according to the chosen metric, regarding the actual value `y`: $(\hat{y} - y)^2$.

Now that we have this pipeline constructed, we must simply start at the end (on the `Cost`), and reach the start `z = wx+b`, to obtain the sought-after parameters (`w` and `b`). To get to `w`, we use the chain rule, starting from the `Cost`, and going to the prediction (the neuron's activation function output, `ŷ = a(z)`). Then, from the predicted output `ŷ`, to the activation's functions input, `z`. And then, finally, we reach the desired variable `w`. In each of these chain steps, when I say, get from J to i, what I formally mean is to get the gradient of `J` with respect to `i`, formally, $\frac{∂J}{∂i}$. So, to get what we are seeking, we must simply do $\frac{∂C}{∂w}=\frac{∂C}{∂ŷ}\frac{∂ŷ}{∂z}\frac{∂z}{∂w}$ for the weight, and $\frac{∂C}{∂w}=\frac{∂C}{∂ŷ}\frac{∂ŷ}{∂z}\frac{∂z}{∂w}$ for the bias.

  

Remember, this is only for a single neuron, so in a common network's environment, we need to aggregate the gradients of all neurons on the last layer in some way (e.g. averaging them).

To make the neuron learn (at each step, slightly better) weight and bias values, we update them, considering the gradient of our cost function, which will decide the size of the step to take. We should also pay attention to a special value called the learning rate. This small number is what we call a hyperparameter, which influences how rapidly the network adapts. All things considered, let's just say that normally we would take big steps in the beginning, and smaller steps as we reach lower Cost function values, optimizing the parameters of our network. Doing this iteratively, we can reach a point where the parameters we were seeking (weights and biases), allow us to very accurately deduce any parameterized function we intend to, and consequently, with some extra effort, solve fairly complex problems.